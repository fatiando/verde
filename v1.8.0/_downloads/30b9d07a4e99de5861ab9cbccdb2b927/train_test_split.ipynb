{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Splitting data into train and test sets\n\nVerde gridders are mostly linear models that are used to predict data at new\nlocations. As such, they are subject to *over-fitting* and we should always\nstrive to quantify the quality of the model predictions (see\n`model_evaluation`). Common practice for\ndoing this is to split the data into training (the one that is used to fit the\nmodel) and testing (the one that is used to validate the predictions) datasets.\n\nThese two datasets can be generated by splitting the data randomly (without\nregard for their positions in space). This is the default behaviour of function\n:func:`verde.train_test_split`, which is based on the scikit-learn function\n:func:`sklearn.model_selection.train_test_split`. This can be problematic if\nthe data points are autocorrelated (values close to each other spatially tend\nto have similar values). In these cases, splitting the data randomly can\noverestimate the prediction quality [Roberts_etal2017]_.\n\nAlternatively, Verde allows splitting the data along *spatial blocks*. In this\ncase, the data are first grouped into blocks with a given size and then the\nblocks are split randomly between training and testing sets.\n\nThis example compares splitting our sample dataset using both methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\nimport verde as vd\n\n# Let's split the Baja California shipborne bathymetry data\ndata = vd.datasets.fetch_baja_bathymetry()\ncoordinates = (data.longitude, data.latitude)\nvalues = data.bathymetry_m\n\n# Assign 20% of the data to the testing set.\ntest_size = 0.2\n\n# Split the data randomly into training and testing. Set the random state\n# (seed) so that we get the same result if running this code again.\ntrain, test = vd.train_test_split(\n    coordinates, values, test_size=test_size, random_state=123\n)\n# train and test are tuples = (coordinates, data, weights).\nprint(\"Train and test size for random splits:\", train[0][0].size, test[0][0].size)\n\n# A different strategy is to first assign the data to blocks and then split the\n# blocks randomly. To do this, specify the size of the blocks using the\n# 'spacing' argument.\ntrain_block, test_block = vd.train_test_split(\n    coordinates,\n    values,\n    spacing=10 / 60,\n    test_size=test_size,\n    random_state=213,\n)\n# Verde will automatically attempt to balance the data between the splits so\n# that the desired amount is assigned to the test set. It won't be exact since\n# blocks contain different amounts of data points.\nprint(\n    \"Train and test size for block splits: \",\n    train_block[0][0].size,\n    test_block[0][0].size,\n)\n\n# Cartopy requires setting the coordinate reference system (CRS) of the\n# original data through the transform argument. Their docs say to use\n# PlateCarree to represent geographic data.\ncrs = ccrs.PlateCarree()\n\n# Make Mercator maps of the two different ways of splitting\nfig, (ax1, ax2) = plt.subplots(\n    1, 2, figsize=(10, 6), subplot_kw=dict(projection=ccrs.Mercator())\n)\n\n# Use an utility function to setup the tick labels and the land feature\nvd.datasets.setup_baja_bathymetry_map(ax1)\nvd.datasets.setup_baja_bathymetry_map(ax2)\n\nax1.set_title(\"Random splitting\")\nax1.plot(*train[0], \".b\", markersize=2, transform=crs, label=\"Train\")\nax1.plot(*test[0], \".r\", markersize=2, transform=crs, label=\"Test\", alpha=0.5)\n\nax2.set_title(\"Blocked random splitting\")\nax2.plot(*train_block[0], \".b\", markersize=2, transform=crs, label=\"Train\")\nax2.plot(*test_block[0], \".r\", markersize=2, transform=crs, label=\"Test\")\nax2.legend(loc=\"upper right\")\n\nplt.subplots_adjust(wspace=0.15, top=1, bottom=0, left=0.05, right=0.95)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}