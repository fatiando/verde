{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Evaluating Performance\n\nThe Green's functions based interpolations in Verde are all linear regressions\nunder the hood. This means that we can use some of the same tactics from\n:mod:`sklearn.model_selection` to evaluate our interpolator's performance. Once\nwe have a quantified measure of the quality of a given fitted gridder, we can\nuse it to tune the gridder's parameters, like ``damping`` for a\n:class:`~verde.Spline` (see `model_selection`).\n\nVerde provides adaptations of common scikit-learn tools to work better with\nspatial data. Let's use these tools to evaluate the performance of a\n:class:`~verde.Spline` on our sample air temperature data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cartopy.crs as ccrs\nimport dask\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pyproj\nfrom sklearn.model_selection import ShuffleSplit\n\nimport verde as vd\n\ndata = vd.datasets.fetch_texas_wind()\n\n# Use Mercator projection because Spline is a Cartesian gridder\nprojection = pyproj.Proj(proj=\"merc\", lat_ts=data.latitude.mean())\nproj_coords = projection(data.longitude.values, data.latitude.values)\n\nregion = vd.get_region((data.longitude, data.latitude))\n# For this data, we'll generate a grid with 15 arc-minute spacing\nspacing = 15 / 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting the data\n\nWe can't evaluate a gridder on the data that went into fitting it. The true\ntest of a model is if it can correctly predict data that it hasn't seen\nbefore. scikit-learn has the :func:`sklearn.model_selection.train_test_split`\nfunction to separate a dataset into two parts: one for fitting the model\n(called *training* data) and a separate one for evaluating the model (called\n*testing* data). Using it with spatial data would involve some tedious array\nconversions so Verde implements :func:`verde.train_test_split` which does the\nsame thing but takes coordinates and data arrays instead.\n\nThe split is done randomly so we specify a seed for the random number\ngenerator to guarantee that we'll get the same result every time we run this\nexample. You probably don't want to do that for real data. We'll keep 30% of\nthe data to use for testing (``test_size=0.3``).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train, test = vd.train_test_split(\n    proj_coords, data.air_temperature_c, test_size=0.3, random_state=0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The returned ``train`` and ``test`` variables are tuples containing\ncoordinates, data, and (optionally) weights arrays. Since we're not using\nweights, the third element of the tuple will be ``None``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot these two datasets with different colors:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\nax = plt.axes()\nax.set_title(\"Air temperature measurements for Texas\")\nax.plot(train[0][0], train[0][1], \".r\", label=\"train\")\nax.plot(test[0][0], test[0][1], \".b\", label=\"test\")\nax.legend()\nax.set_aspect(\"equal\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can pass the training dataset to the :meth:`~verde.base.BaseGridder.fit`\nmethod of most gridders using Python's argument expansion using the ``*``\nsymbol.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline = vd.Spline()\nspline.fit(*train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the gridded result to see what it looks like. First, we'll create\na geographic grid:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid = spline.grid(\n    region=region,\n    spacing=spacing,\n    projection=projection,\n    dims=[\"latitude\", \"longitude\"],\n    data_names=\"temperature\",\n)\nprint(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we'll mask out grid points that are too far from any given data point\nand plot the grid:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = vd.distance_mask(\n    (data.longitude, data.latitude),\n    maxdist=3 * spacing * 111e3,\n    coordinates=vd.grid_coordinates(region, spacing=spacing),\n    projection=projection,\n)\ngrid = grid.where(mask)\n\nplt.figure(figsize=(8, 6))\nax = plt.axes(projection=ccrs.Mercator())\nax.set_title(\"Gridded temperature\")\npc = grid.temperature.plot.pcolormesh(\n    ax=ax,\n    cmap=\"plasma\",\n    transform=ccrs.PlateCarree(),\n    add_colorbar=False,\n    add_labels=False,\n)\nplt.colorbar(pc).set_label(\"C\")\nax.plot(data.longitude, data.latitude, \".k\", markersize=1, transform=ccrs.PlateCarree())\nvd.datasets.setup_texas_wind_map(ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scoring\n\nGridders in Verde implement the :meth:`~verde.base.BaseGridder.score` method\nthat calculates the `R\u00b2 coefficient of determination\n<https://en.wikipedia.org/wiki/Coefficient_of_determination>`__ for a given\ncomparison dataset (``test`` in our case). The R\u00b2 score is at most 1, meaning\na perfect prediction, but has no lower bound.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = spline.score(*test)\nprint(\"R\u00b2 score:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's a good score meaning that our gridder is able to accurately predict\ndata that wasn't used in the gridding algorithm.\n\n.. caution::\n\n    Once caveat for this score is that it is highly dependent on the\n    particular split that we made. Changing the random number generator seed\n    in :func:`verde.train_test_split` will result in a different score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use 1 as a seed instead of 0\ntrain_other, test_other = vd.train_test_split(\n    proj_coords, data.air_temperature_c, test_size=0.3, random_state=1\n)\n\nprint(\"R\u00b2 score with seed 1:\", vd.Spline().fit(*train_other).score(*test_other))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-validation\n\nA more robust way of scoring the gridders is to use function\n:func:`verde.cross_val_score`, which (by default) uses a `k-fold\ncross-validation\n<https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation>`__\nby default. It will split the data *k* times and return the score on each\n*fold*. We can then take a mean of these scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = vd.cross_val_score(vd.Spline(), proj_coords, data.air_temperature_c)\nprint(\"k-fold scores:\", scores)\nprint(\"Mean score:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use most cross-validation splitter classes from\n:mod:`sklearn.model_selection` by specifying the ``cv`` argument. For\nexample, if we want to shuffle then split the data *n* times\n(:class:`sklearn.model_selection.ShuffleSplit`):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shuffle = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n\nscores = vd.cross_val_score(\n    vd.Spline(), proj_coords, data.air_temperature_c, cv=shuffle\n)\nprint(\"shuffle scores:\", scores)\nprint(\"Mean score:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel cross-validation\n\nCross-validation involves running several model fit and score operations\nwhich are independent of each other. Because of this, they are prime targets\nfor parallelization. Verde uses the excellent `Dask <https://dask.org/>`__\nlibrary for parallel execution.\n\nTo run :func:`verde.cross_val_score` with Dask, use the ``delayed`` argument:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = vd.cross_val_score(\n    vd.Spline(), proj_coords, data.air_temperature_c, delayed=True\n)\nprint(\"Delayed k-fold scores:\", scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the scores haven't actually been computed yet (hence the\n\"delayed\" term). Instead, Verde scheduled the operations with Dask. Since we\nare interested only in the mean score, we can schedule the mean as well using\n:func:`dask.delayed`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_score = dask.delayed(np.mean)(scores)\nprint(\"Delayed mean:\", mean_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run the scheduled computations and get the mean score, use\n:func:`dask.compute` or ``.compute()``. Dask will automatically execute\nthings in parallel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_score = mean_score.compute()\nprint(\"Mean score:\", mean_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Dask will run many ``fit`` operations in parallel, which can be memory\n    intensive. Make sure you have enough RAM to run multiple fits.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improving the score\n\nThat score is not bad but it could be better. The default arguments for\n:class:`~verde.Spline` aren't optimal for this dataset. We could try\ndifferent combinations manually until we get a good score. A better way is to\ndo this automatically. In `model_selection` we'll go over how to do just\nthat.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}