{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Model Selection\n\nIn `model_evaluation`, we saw how to check the performance of an\ninterpolator using cross-validation. We found that the default parameters for\n:class:`verde.Spline` are not good for predicting our sample air temperature\ndata. Now, let's see how we can tune the :class:`~verde.Spline` to improve the\ncross-validation performance.\n\nOnce again, we'll start by importing the required packages and loading our\nsample data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pyproj\n\nimport verde as vd\n\ndata = vd.datasets.fetch_texas_wind()\n\n# Use Mercator projection because Spline is a Cartesian gridder\nprojection = pyproj.Proj(proj=\"merc\", lat_ts=data.latitude.mean())\nproj_coords = projection(data.longitude.values, data.latitude.values)\n\nregion = vd.get_region((data.longitude, data.latitude))\n# The desired grid spacing in degrees\nspacing = 15 / 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we begin tuning, let's reiterate what the results were with the\ndefault parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline_default = vd.Spline()\nscore_default = np.mean(\n    vd.cross_val_score(spline_default, proj_coords, data.air_temperature_c)\n)\nspline_default.fit(proj_coords, data.air_temperature_c)\nprint(\"R\u00b2 with defaults:\", score_default)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning\n\n:class:`~verde.Spline` has many parameters that can be set to modify the\nfinal result. Mainly the ``damping`` regularization parameter and the\n``mindist`` \"fudge factor\" which smooths the solution. Would changing the\ndefault values give us a better score?\n\nWe can answer these questions by changing the values in our ``spline`` and\nre-evaluating the model score repeatedly for different values of these\nparameters. Let's test the following combinations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dampings = [None, 1e-4, 1e-3, 1e-2]\nmindists = [5e3, 10e3, 50e3, 100e3]\n\n# Use itertools to create a list with all combinations of parameters to test\nparameter_sets = [\n    dict(damping=combo[0], mindist=combo[1])\n    for combo in itertools.product(dampings, mindists)\n]\nprint(\"Number of combinations:\", len(parameter_sets))\nprint(\"Combinations:\", parameter_sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can loop over the combinations and collect the scores for each\nparameter set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline = vd.Spline()\nscores = []\nfor params in parameter_sets:\n    spline.set_params(**params)\n    score = np.mean(vd.cross_val_score(spline, proj_coords, data.air_temperature_c))\n    scores.append(score)\nprint(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The largest score will yield the best parameter combination.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best = np.argmax(scores)\nprint(\"Best score:\", scores[best])\nprint(\"Score with defaults:\", score_default)\nprint(\"Best parameters:\", parameter_sets[best])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**That is a nice improvement over our previous score!**\n\nThis type of tuning is important and should always be performed when using a\nnew gridder or a new dataset. However, the above implementation requires a\nlot of coding. Fortunately, Verde provides convenience classes that perform\nthe cross-validation and tuning automatically when fitting a dataset.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-validated gridders\n\nThe :class:`verde.SplineCV` class provides a cross-validated version of\n:class:`verde.Spline`. It has almost the same interface but does all of the\nabove automatically when fitting a dataset. The only difference is that you\nmust provide a list of ``damping`` and ``mindist`` parameters to try instead\nof only a single value:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline = vd.SplineCV(\n    dampings=dampings,\n    mindists=mindists,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calling :meth:`~verde.SplineCV.fit` will run a grid search over all parameter\ncombinations to find the one that maximizes the cross-validation score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline.fit(proj_coords, data.air_temperature_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The estimated best ``damping`` and ``mindist``, as well as the\ncross-validation scores, are stored in class attributes:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Highest score:\", spline.scores_.max())\nprint(\"Best damping:\", spline.damping_)\nprint(\"Best mindist:\", spline.mindist_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cross-validated gridder can be used like any other gridder (including in\n:class:`verde.Chain` and :class:`verde.Vector`):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid = spline.grid(\n    region=region,\n    spacing=spacing,\n    projection=projection,\n    dims=[\"latitude\", \"longitude\"],\n    data_names=\"temperature\",\n)\nprint(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like :func:`verde.cross_val_score`, :class:`~verde.SplineCV` can also run the\ngrid search in parallel using `Dask <https://dask.org/>`__ by specifying the\n``delayed`` attribute:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline = vd.SplineCV(dampings=dampings, mindists=mindists, delayed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unlike :func:`verde.cross_val_score`, calling :meth:`~verde.SplineCV.fit`\ndoes **not** result in :func:`dask.delayed` objects. The full grid search is\nexecuted and the optimal parameters are found immediately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline.fit(proj_coords, data.air_temperature_c)\n\nprint(\"Best damping:\", spline.damping_)\nprint(\"Best mindist:\", spline.mindist_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The one caveat is the that the ``scores_`` attribute will be a list of\n:func:`dask.delayed` objects instead because the scores are only computed as\nintermediate values in the scheduled computations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Delayed scores:\", spline.scores_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calling :func:`dask.compute` on the scores will calculate their values but\nwill unfortunately run the entire grid search again. So using\n``delayed=True`` is not recommended if you need the scores of each parameter\ncombination.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The importance of tuning\n\nTo see the difference that tuning has on the results, we can make a grid\nwith the best configuration and see how it compares to the default result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid_default = spline_default.grid(\n    region=region,\n    spacing=spacing,\n    projection=projection,\n    dims=[\"latitude\", \"longitude\"],\n    data_names=\"temperature\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot our grids side-by-side:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = vd.distance_mask(\n    (data.longitude, data.latitude),\n    maxdist=3 * spacing * 111e3,\n    coordinates=vd.grid_coordinates(region, spacing=spacing),\n    projection=projection,\n)\n\ngrid = grid.where(mask)\ngrid_default = grid_default.where(mask)\n\nplt.figure(figsize=(14, 8))\nfor i, title, grd in zip(range(2), [\"Defaults\", \"Tuned\"], [grid_default, grid]):\n    ax = plt.subplot(1, 2, i + 1, projection=ccrs.Mercator())\n    ax.set_title(title)\n    pc = grd.temperature.plot.pcolormesh(\n        ax=ax,\n        cmap=\"plasma\",\n        transform=ccrs.PlateCarree(),\n        vmin=data.air_temperature_c.min(),\n        vmax=data.air_temperature_c.max(),\n        add_colorbar=False,\n        add_labels=False,\n    )\n    plt.colorbar(pc, orientation=\"horizontal\", aspect=50, pad=0.05).set_label(\"C\")\n    ax.plot(\n        data.longitude, data.latitude, \".k\", markersize=1, transform=ccrs.PlateCarree()\n    )\n    vd.datasets.setup_texas_wind_map(ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that, for sparse data like these, **smoother models tend to be better\npredictors**. This is a sign that you should probably not trust many of the\nshort wavelength features that we get from the defaults.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}