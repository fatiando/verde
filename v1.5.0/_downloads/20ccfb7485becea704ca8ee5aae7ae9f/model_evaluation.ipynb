{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nEvaluating Performance\n======================\n\nThe Green's functions based interpolations in Verde are all linear regressions under the\nhood. This means that we can use some of the same tactics from\n:mod:`sklearn.model_selection` to evaluate our interpolator's performance. Once we have\na quantified measure of the quality of a given fitted gridder, we can use it to tune the\ngridder's parameters, like ``damping`` for a :class:`~verde.Spline` (see\n`model_selection`).\n\nVerde provides adaptations of common scikit-learn tools to work better with spatial\ndata. Let's use these tools to evaluate the performance of a :class:`~verde.Spline` on\nour sample air temperature data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport pyproj\nimport verde as vd\n\ndata = vd.datasets.fetch_texas_wind()\n\n# Use Mercator projection because Spline is a Cartesian gridder\nprojection = pyproj.Proj(proj=\"merc\", lat_ts=data.latitude.mean())\nproj_coords = projection(data.longitude.values, data.latitude.values)\n\nregion = vd.get_region((data.longitude, data.latitude))\n# For this data, we'll generate a grid with 15 arc-minute spacing\nspacing = 15 / 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the data\n------------------\n\nWe can't evaluate a gridder on the data that went into fitting it. The true test of a\nmodel is if it can correctly predict data that it hasn't seen before. scikit-learn has\nthe :func:`sklearn.model_selection.train_test_split` function to separate a dataset\ninto two parts: one for fitting the model (called *training* data) and a separate one\nfor evaluating the model (called *testing* data). Using it with spatial data would\ninvolve some tedious array conversions so Verde implements\n:func:`verde.train_test_split` which does the same thing but takes coordinates and\ndata arrays instead.\n\nThe split is done randomly so we specify a seed for the random number generator to\nguarantee that we'll get the same result every time we run this example. You probably\ndon't want to do that for real data. We'll keep 30% of the data to use for testing\n(``test_size=0.3``).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train, test = vd.train_test_split(\n    proj_coords, data.air_temperature_c, test_size=0.3, random_state=0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The returned ``train`` and ``test`` variables are tuples containing coordinates, data,\nand (optionally) weights arrays. Since we're not using weights, the third element of\nthe tuple will be ``None``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot these two datasets with different colors:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\nax = plt.axes()\nax.set_title(\"Air temperature measurements for Texas\")\nax.plot(train[0][0], train[0][1], \".r\", label=\"train\")\nax.plot(test[0][0], test[0][1], \".b\", label=\"test\")\nax.legend()\nax.set_aspect(\"equal\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can pass the training dataset to the :meth:`~verde.base.BaseGridder.fit` method of\nmost gridders using Python's argument expansion using the ``*`` symbol.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline = vd.Spline()\nspline.fit(*train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the gridded result to see what it looks like. First, we'll create a\ngeographic grid:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid = spline.grid(\n    region=region,\n    spacing=spacing,\n    projection=projection,\n    dims=[\"latitude\", \"longitude\"],\n    data_names=[\"temperature\"],\n)\nprint(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we'll mask out grid points that are too far from any given data point and plot\nthe grid:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = vd.distance_mask(\n    (data.longitude, data.latitude),\n    maxdist=3 * spacing * 111e3,\n    coordinates=vd.grid_coordinates(region, spacing=spacing),\n    projection=projection,\n)\ngrid = grid.where(mask)\n\nplt.figure(figsize=(8, 6))\nax = plt.axes(projection=ccrs.Mercator())\nax.set_title(\"Gridded temperature\")\npc = grid.temperature.plot.pcolormesh(\n    ax=ax,\n    cmap=\"plasma\",\n    transform=ccrs.PlateCarree(),\n    add_colorbar=False,\n    add_labels=False,\n)\nplt.colorbar(pc).set_label(\"C\")\nax.plot(data.longitude, data.latitude, \".k\", markersize=1, transform=ccrs.PlateCarree())\nvd.datasets.setup_texas_wind_map(ax)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scoring\n--------\n\nGridders in Verde implement the :meth:`~verde.base.BaseGridder.score` method that\ncalculates the `R\u00b2 coefficient of determination\n<https://en.wikipedia.org/wiki/Coefficient_of_determination>`__\nfor a given comparison dataset (``test`` in our case). The R\u00b2 score is at most 1,\nmeaning a perfect prediction, but has no lower bound.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = spline.score(*test)\nprint(\"R\u00b2 score:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's a good score meaning that our gridder is able to accurately predict data that\nwasn't used in the gridding algorithm.\n\n.. caution::\n\n    Once caveat for this score is that it is highly dependent on the particular split\n    that we made. Changing the random number generator seed in\n    :func:`verde.train_test_split` will result in a different score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use 1 as a seed instead of 0\ntrain_other, test_other = vd.train_test_split(\n    proj_coords, data.air_temperature_c, test_size=0.3, random_state=1\n)\n\nprint(\"R\u00b2 score with seed 1:\", vd.Spline().fit(*train_other).score(*test_other))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-validation\n----------------\n\nA more robust way of scoring the gridders is to use function\n:func:`verde.cross_val_score`, which (by default) uses a `k-fold cross-validation\n<https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation>`__\nby default. It will split the data *k* times and return the score on each *fold*. We\ncan then take a mean of these scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = vd.cross_val_score(vd.Spline(), proj_coords, data.air_temperature_c)\nprint(\"k-fold scores:\", scores)\nprint(\"Mean score:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use most cross-validation splitter classes from\n:mod:`sklearn.model_selection` by specifying the ``cv`` argument. For example, if we\nwant to shuffle then split the data *n* times\n(:class:`sklearn.model_selection.ShuffleSplit`):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n\nshuffle = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n\nscores = vd.cross_val_score(\n    vd.Spline(), proj_coords, data.air_temperature_c, cv=shuffle\n)\nprint(\"shuffle scores:\", scores)\nprint(\"Mean score:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parallel cross-validation\n-------------------------\n\nCross-validation involves running several model fit and score operations\nwhich are independent of each other. Because of this, they are prime targets\nfor parallelization. Verde uses the excellent `Dask <https://dask.org/>`__\nlibrary for parallel execution.\n\nTo run :func:`verde.cross_val_score` with Dask, use the ``delayed`` argument:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = vd.cross_val_score(\n    vd.Spline(), proj_coords, data.air_temperature_c, delayed=True\n)\nprint(\"Delayed k-fold scores:\", scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the scores haven't actually been computed yet (hence the\n\"delayed\" term). Instead, Verde scheduled the operations with Dask. Since we\nare interested only in the mean score, we can schedule the mean as well using\n:func:`dask.delayed`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dask\n\nmean_score = dask.delayed(np.mean)(scores)\nprint(\"Delayed mean:\", mean_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run the scheduled computations and get the mean score, use\n:func:`dask.compute` or ``.compute()``. Dask will automatically execute\nthings in parallel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_score = mean_score.compute()\nprint(\"Mean score:\", mean_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Dask will run many ``fit`` operations in parallel, which can be memory\n    intensive. Make sure you have enough RAM to run multiple fits.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Improving the score\n-------------------\n\nThat score is not bad but it could be better. The default arguments for\n:class:`~verde.Spline` aren't optimal for this dataset. We could try\ndifferent combinations manually until we get a good score. A better way is to\ndo this automatically. In `model_selection` we'll go over how to do just\nthat.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}